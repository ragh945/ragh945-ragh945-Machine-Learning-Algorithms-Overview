{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941c785b-03df-4512-8512-687fc617aad5",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "- **Principal Component Analysis (PCA)** is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining most of the variance present in the data. It achieves this by transforming the data into a new set of features called **principal components**, which are orthogonal and capture the maximum variance in the data.\n",
    "\n",
    "## How PCA Works:\n",
    "- **Standardize the Data**: Since PCA is affected by the scale of features, the data is usually standardized to have a mean of 0 and a variance of 1.\n",
    "- **Compute the Covariance Matrix**: The covariance matrix describes the relationship between features in the data.\n",
    "- **Compute the Eigenvalues and Eigenvectors**: Eigenvectors determine the direction of the principal components, and eigenvalues determine the magnitude (variance) captured by these components.\n",
    "- **Sort and Select Principal Components**: The eigenvectors are sorted in descending order of their corresponding eigenvalues. The top k eigenvectors are selected to form the new feature space (where k is the number of desired components).\n",
    "- **Transform the Data**: The original data is projected onto the selected principal components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bc2bf-a880-4016-a091-0d38ea891db8",
   "metadata": {},
   "source": [
    "# Advantages of PCA\n",
    "- **Reduces Dimensionality**: PCA helps reduce the number of features in the dataset, making it easier and faster to process, especially for large datasets.\n",
    "- **Improves Performance**: By removing redundant or correlated features, PCA can improve the performance of machine learning models by reducing overfitting.\n",
    "- **Captures Most Variance**: PCA retains the most important information by preserving the maximum variance, ensuring that the key patterns in the data are kept.\n",
    "- **Enhances Visualization**: PCA helps visualize high-dimensional data by reducing it to 2 or 3 principal components, making it easier to interpret and understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeabe68-5c79-485e-9fa7-4c51757a8734",
   "metadata": {},
   "source": [
    "| Aspect                   | Feature Selection                            | Principal Component Analysis (PCA)          |\r\n",
    "|--------------------------|----------------------------------------------|---------------------------------------------|\r\n",
    "| **Definition**            | Choosing a subset of original features.      | Creating new features by combining original ones. |\r\n",
    "| **Purpose**               | To retain the most important features.       | To reduce the number of dimensions while retaining the variance. |\r\n",
    "| **Process**               | Eliminates irrelevant or redundant features. | Transforms original features into new uncorrelated components. |\r\n",
    "| **Output**                | A smaller subset of the original features.   | New components that are linear combinations of the original features. |\r\n",
    "| **Interpretability**      | Retained features are easy to interpret.     | New components may not have an intuitive interpretation. |\r\n",
    "| **Preserving Variance**   | Does not guarantee variance preservation.     | Maximizes variance along the new components. |\r\n",
    "| **Example**               | Selecting the top 5 features based on correlation or importance. | Reducing from 10 features to 3 components that capture most variance. |\r\n",
    "| **Use Case**              | When you want to keep original features.     | When you want to reduce dimensions and capture the most variance. |\r\n",
    "| **Impact on Model**       | Direct impact by removing irrelevant features. | Indirect impact by changing the feature space. |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258761a-988c-4b60-a800-af0d303a40fb",
   "metadata": {},
   "source": [
    "## Python Implementation of PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef0c086-b1fc-4b3e-8679-7fca79f2b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix:\n",
      " [[0.66666667 0.66666667 0.66666667 0.66666667]\n",
      " [0.66666667 1.66666667 1.66666667 1.66666667]\n",
      " [0.66666667 1.66666667 1.66666667 1.66666667]\n",
      " [0.66666667 1.66666667 1.66666667 1.66666667]]\n",
      "Eigenvalues:\n",
      " [3.78180023e-01 5.28848664e+00 1.68501120e-32 1.36845553e-48]\n",
      "Eigenvectors:\n",
      " [[-9.70179752e-01 -2.42386568e-01  7.85046229e-17 -1.22607339e-32]\n",
      " [ 1.39941950e-01 -5.60133541e-01  8.16496581e-01  1.54125491e-17]\n",
      " [ 1.39941950e-01 -5.60133541e-01 -4.08248290e-01 -7.07106781e-01]\n",
      " [ 1.39941950e-01 -5.60133541e-01 -4.08248290e-01  7.07106781e-01]]\n",
      "PCA Transformed Data:\n",
      " [[-0.84020031  0.20991293]\n",
      " [-2.7629875  -0.34044098]\n",
      " [ 1.08258688  0.76026683]\n",
      " [ 2.52060094 -0.62973878]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Create the DataFrame\n",
    "data = {\n",
    "    'Target': [0, 1, 0, 1],\n",
    "    'Feature 1': [2, 1, 3, 2],\n",
    "    'Feature 2': [4, 3, 5, 6],\n",
    "    'Feature 3': [6, 5, 7, 8],\n",
    "    'Feature 4': [8, 7, 9, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Extract features and mean-center them\n",
    "X = df[['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']].values\n",
    "X_mean = X - np.mean(X, axis=0)\n",
    "\n",
    "# Step 3: Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X_mean, rowvar=False)\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "# Step 4: Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# Step 5: Perform PCA using sklearn\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"PCA Transformed Data:\\n\", X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a1b308-9c03-4f5b-98cd-bf3f5acc98cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Transformed DataFrame:\n",
      "    Principal Component 1  Principal Component 2  Target\n",
      "0              -0.840200               0.209913       0\n",
      "1              -2.762988              -0.340441       1\n",
      "2               1.082587               0.760267       0\n",
      "3               2.520601              -0.629739       1\n"
     ]
    }
   ],
   "source": [
    "# Convert PCA transformed data into a DataFrame\n",
    "pca_df = pd.DataFrame(X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Include the target variable for reference\n",
    "pca_df['Target'] = df['Target']\n",
    "\n",
    "print(\"PCA Transformed DataFrame:\\n\", pca_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
